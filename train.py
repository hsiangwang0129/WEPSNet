import os
import argparse
import time
import shutil

import torch
import torch.utils.data as data
import torch.backends.cudnn as cudnn

from torchvision import transforms
from data_loader import get_segmentation_dataset
from models.fast_scnn import get_fast_scnn
from utils.loss import MixSoftmaxCrossEntropyOHEMLoss
from utils.lr_scheduler import LRScheduler
from utils.metric import SegmentationMetric


def find_latest_checkpoint(save_folder, model_name, dataset_name):
    """è‡ªå‹•å°‹æ‰¾æœ€æ–°çš„ Checkpoint"""
    latest_checkpoint = os.path.join(save_folder, f"{model_name}_{dataset_name}_latest.pth")
    if os.path.exists(latest_checkpoint):
        print(f"ğŸ“¥ æ‰¾åˆ°æœ€æ–° checkpoint: {latest_checkpoint}")
        return latest_checkpoint
    return None


def parse_args():
    """Training Options for Segmentation Experiments"""
    parser = argparse.ArgumentParser(description='Fast-SCNN on PyTorch')
    parser.add_argument('--model', type=str, default='fast_scnn', help='model name')
    parser.add_argument('--dataset', type=str, default='citys', help='dataset name')
    parser.add_argument('--base-size', type=int, default=1024, help='base image size')
    parser.add_argument('--crop-size', type=int, default=768, help='crop image size')
    parser.add_argument('--epochs', type=int, default=160, help='number of epochs to train')
    parser.add_argument('--batch-size', type=int, default=2, help='input batch size for training')
    parser.add_argument('--lr', type=float, default=1e-2, help='learning rate')
    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')
    parser.add_argument('--weight-decay', type=float, default=1e-4, help='weight decay')
    parser.add_argument('--save-folder', default='./weights', help='Directory for saving checkpoints')
    parser.add_argument('--resume', type=str, default=None, help='Path to checkpoint file')
    
    args = parser.parse_args()

    # è‡ªå‹•å°‹æ‰¾æœ€æ–°çš„ Checkpoint
    if args.resume is None:
        latest_checkpoint = find_latest_checkpoint(args.save_folder, args.model, args.dataset)
        if latest_checkpoint:
            args.resume = latest_checkpoint

    args.best_pred = 0.0
    args.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    cudnn.benchmark = True
    print(args)
    return args


class Trainer:
    def __init__(self, args):
        self.args = args
        self.device = args.device
        cudnn.benchmark = True

        # è³‡æ–™å¢å¼·èˆ‡æ¨™æº–åŒ–
        input_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([.485, .456, .406], [.229, .224, .225]),
        ])

        # å»ºç«‹è¨“ç·´ & é©—è­‰è³‡æ–™é›†
        data_kwargs = {'transform': input_transform, 'base_size': args.base_size, 'crop_size': args.crop_size}
        train_dataset = get_segmentation_dataset(args.dataset, split='train', mode='train', **data_kwargs)
        val_dataset = get_segmentation_dataset(args.dataset, split='val', mode='val', **data_kwargs)

        self.train_loader = data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)
        self.val_loader = data.DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)

        # å‰µå»ºæ¨¡å‹
        self.model = get_fast_scnn(dataset=args.dataset, aux=False).to(self.device)

        # å„ªåŒ–å™¨ & Loss
        self.criterion = MixSoftmaxCrossEntropyOHEMLoss(aux=False, aux_weight=0.4).to(self.device)
        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
        self.lr_scheduler = LRScheduler(mode='poly', base_lr=args.lr, nepochs=args.epochs, iters_per_epoch=len(self.train_loader), power=0.9)

        # è©•ä¼°æŒ‡æ¨™
        self.metric = SegmentationMetric(train_dataset.num_class)
        self.best_pred = 0.0
        self.start_epoch = 0

        # å¦‚æœæœ‰ checkpointï¼Œè¼‰å…¥æ¨¡å‹
        if args.resume:
            self.load_checkpoint(args.resume)

    def load_checkpoint(self, checkpoint_path):
        """è¼‰å…¥è¨“ç·´é€²åº¦"""
        if os.path.isfile(checkpoint_path):
            print(f'ğŸ“¥ è¼‰å…¥ checkpoint: {checkpoint_path}')
            
            # âœ… åŠ å…¥ weights_only=Falseï¼Œç¢ºä¿ PyTorch æ­£å¸¸è¼‰å…¥
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)

            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.start_epoch = checkpoint['epoch']
            self.best_pred = checkpoint['best_pred']
            print(f'âœ… æˆåŠŸæ¢å¾©è¨“ç·´ï¼Œå¾ Epoch {self.start_epoch} ç¹¼çºŒï¼')
        else:
            print(f'âš ï¸ æ‰¾ä¸åˆ° checkpoint: {checkpoint_path}ï¼Œå¾é ­é–‹å§‹è¨“ç·´ï¼')

    def train(self):
        """è¨“ç·´æ¨¡å‹"""
        cur_iters = 0
        start_time = time.time()

        for epoch in range(self.start_epoch, self.args.epochs):
            self.model.train()
            for i, (images, targets) in enumerate(self.train_loader):
                cur_lr = self.lr_scheduler(cur_iters)
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = cur_lr

                images, targets = images.to(self.device), targets.to(self.device)
                outputs = self.model(images)
                loss = self.criterion(outputs, targets)

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                cur_iters += 1
                if cur_iters % 10 == 0:
                    print(f'Epoch [{epoch}/{self.args.epochs}] Iter [{i+1}/{len(self.train_loader)}] '
                          f'| lr: {cur_lr:.8f} | Loss: {loss.item():.4f}')

            # å„²å­˜ checkpoint
            self.validation(epoch)

        print("âœ… è¨“ç·´å®Œæˆï¼")

    def validation(self, epoch):
        """é©—è­‰æ¨¡å‹"""
        is_best = False
        self.metric.reset()
        self.model.eval()
        
        for i, (image, target) in enumerate(self.val_loader):
            image = image.to(self.device)
            outputs = self.model(image)
            pred = torch.argmax(outputs[0], 1).cpu().data.numpy()
            self.metric.update(pred, target.numpy())
            pixAcc, mIoU = self.metric.get()
            print(f'Epoch {epoch}, Sample {i+1}, Validation pixAcc: {pixAcc * 100:.3f}%, mIoU: {mIoU * 100:.3f}%')

        new_pred = (pixAcc + mIoU) / 2
        if new_pred > self.best_pred:
            is_best = True
            self.best_pred = new_pred

        save_checkpoint(self.model, self.optimizer, epoch, self.args, self.best_pred, is_best)


def save_checkpoint(model, optimizer, epoch, args, best_pred, is_best=False):
    """å­˜å„²æœ€æ–°å’Œæœ€ä½³ Checkpoint"""
    directory = args.save_folder
    os.makedirs(directory, exist_ok=True)

    latest_path = os.path.join(directory, f'{args.model}_{args.dataset}_latest.pth')
    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'best_pred': best_pred}, latest_path)
    print(f'ğŸ’¾ å·²å„²å­˜æœ€æ–°æ¨¡å‹: {latest_path}')

    if is_best:
        best_path = os.path.join(directory, f'{args.model}_{args.dataset}_best.pth')
        shutil.copyfile(latest_path, best_path)
        print(f'ğŸ† å·²å„²å­˜æœ€ä½³æ¨¡å‹: {best_path}')


if __name__ == '__main__':
    args = parse_args()
    trainer = Trainer(args)
    trainer.train()
